# Soft Robotics & AI â€” Evolution Gym Demo

This repo contains a short, live-demoâ€“ready setup for **soft robotics + AI** using
[**Evolution Gym**](https://evolutiongym.github.io/) and **PPO** (Stable-Baselines3).
It accompanies our slides and talk on how **reinforcement learning** can discover gaits
for **voxel-based soft robots**.

## Repo Contents

```
.
â”œâ”€â”€ train_evogym_better.py          # Improved PPO training (VecNormalize, larger net)
â”œâ”€â”€ train_evogym_save.py            # Simple training that also saves robot.npz
â”œâ”€â”€ play_evogym_saved_norm.py       # Playback using VecNormalize stats (recommended)
â”œâ”€â”€ play_evogym.py                  # Legacy playback script (Gym API) â€“ use Gymnasium versions instead
â”œâ”€â”€ record_evogym_gif.py            # Render a short GIF/MP4 from a trained policy
â”œâ”€â”€ robot.npz                       # Saved robot morphology (generated by training)
â”œâ”€â”€ ppo_evogym_walker.zip           # Trained PPO policy (generated by training)
â”œâ”€â”€ vecnormalize.pkl                # Normalization stats (generated by training)
â”œâ”€â”€ evogym_demo.gif                 # Example animation (optional)
â”œâ”€â”€ Soft_Robotics_AI_Presentation_EvoGym.pptx
â”œâ”€â”€ Soft_Robotics_AI_Speech.pdf
â”œâ”€â”€ Soft_Robotics_AI_Updated_Speech.pdf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> Note: `robot.npz`, `ppo_evogym_walker.zip`, and `vecnormalize.pkl` are **artifacts** produced by training.
> You can delete and regenerate them at any time.

## Quickstart

### 1) Create an environment (Python 3.10 recommended)

Using **conda**:
```bash
conda create -n evogym python=3.10 -y
conda activate evogym
```

### 2) Install dependencies

```bash
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
```

### 3) Train (fast-to-good)

Recommended script (normalization + better defaults):

```bash
python train_evogym_better.py --timesteps 300000 --out ppo_evogym_walker --size 6
```

- **Shorter rehearsal run:** `--timesteps 150000`
- **Longer, stronger gait:** `--timesteps 500000`

This produces:
- `robot.npz` â€” the exact robot body used
- `ppo_evogym_walker.zip` â€” trained policy (the â€œbrainâ€)
- `vecnormalize.pkl` â€” observation/reward normalization statistics

### 4) Replay (with normalization)

```bash
python play_evogym_saved_norm.py
```

This loads the **same robot** and the **VecNormalize** stats to reproduce training behavior faithfully.

### 5) Make a GIF / MP4 (for slides)

```bash
python record_evogym_gif.py --steps 500 --fps 30 --gif evogym_demo.gif --mp4 evogym_demo.mp4
```

## What â€œtrainingâ€ does (in one paragraph)

We place a voxel-based soft robot in an environment like `Walker-v0`. A neural-network **policy** outputs
actuator commands (expand/contract). The simulator returns a **reward** (e.g., forward distance). Using **PPO**
(reinforcement learning), the policy is updated to increase future reward. Over many steps, the robot discovers a **gait**.
The outputs (`ppo_evogym_walker.zip`, `robot.npz`) let you replay the learned behavior.

## PPO Explained

**PPO (Proximal Policy Optimization)** is the reinforcement learning algorithm we use here.

- **Policy optimization**: The â€œbrainâ€ is a policy network (a neural net mapping state â†’ action). PPO tunes this network to maximize reward.  
- **Proximal**: PPO only allows *small, safe updates* (by clipping changes), so the policy doesnâ€™t forget what it already learned.  
- **How it works (loop):**
  1. Collect rollouts (robot interacts with env).
  2. Compute advantages (which actions were better than expected).
  3. Update the policy network with a clipped loss, so changes stay close to the old policy.
  4. Repeat â†’ the robot learns gradually.

- **Why itâ€™s good:** PPO is stable, efficient, and simple. Itâ€™s widely used in robotics and games.  
- **In this demo:** PPO learns actuator patterns that move the voxel-based body forward â€” discovering a gait without us programming it.

ðŸ‘‰ A good way to say it in talks:  
*â€œPPO is like teaching the robot by trial and error, but with training wheels â€” each update is small and safe, so the robot steadily improves without losing what it already knows.â€*

## Troubleshooting

- **Window doesnâ€™t render (macOS):** `pip install pyglet` (already in requirements). Ensure scripts use `render_mode="human"` for live windows or `"rgb_array"` for offscreen capture.
- **Playback mismatch error (action dims):** You trained with a different robot. Use the saved `robot.npz` (already handled by `play_evogym_saved_norm.py`), or retrain and keep artifacts together.
- **Slow learning:** Increase `--timesteps` (300kâ†’500k), reduce body size (`--size 5`), or lower LR to `1e-4`. You can also parallelize envs in `train_evogym_better.py` (2Ã— `DummyVecEnv`).

## Create & Push a GitHub Repo

1. **On GitHub:** Create a new repo (e.g., `soft-robotics-ai-evogym`) â€” no need to initialize with README.
2. **In this folder:**
   ```bash
   git init
   git branch -M main
   echo "__pycache__/
*.pyc
*.pkl
*.zip
*.gif
*.mp4
.env
.venv
*egg-info/
.DS_Store
" > .gitignore
   git add .
   git commit -m "Soft Robotics & AI demo: Evolution Gym + PPO"
   ```
3. **Add remote & push (choose HTTPS or SSH):**
   ```bash
   # HTTPS (replace USER/REPO)
   git remote add origin https://github.com/USER/soft-robotics-ai-evogym.git
   git push -u origin main

   # or SSH
   git remote add origin git@github.com:USER/soft-robotics-ai-evogym.git
   git push -u origin main
   ```

> If any files exceed 100MB (unlikely here), use **Git LFS**:
> `git lfs install && git lfs track "*.zip" "*.mp4"`

## Acknowledgments

- **Evolution Gym** authors and maintainers
- **Stable-Baselines3** team
- Inspiration from bio-inspired soft robotics research communities

---

Happy experimenting! If you hit snags, open an issue or ping us.
